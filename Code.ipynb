{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0,'/home/prashant/Imputer.py/imputer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Train.csv')\n",
    "test = pd.read_csv('./Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Item_Type_Combined'] = train['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "train['Item_Type_Combined'] = train['Item_Type_Combined'].map({'FD':'Food','NC':'Non-Consumable','DR':'Drink'})\n",
    "train.loc[train['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content'] = \"Non-Edible\"\n",
    "test['Item_Type_Combined'] = test['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "test['Item_Type_Combined'] = test['Item_Type_Combined'].map({'FD':'Food','NC':'Non-Consumable','DR':'Drink'})\n",
    "test.loc[test['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content'] = \"Non-Edible\"\n",
    "combine = [train,test]\n",
    "type_dict=list(train['Item_Type'].value_counts().index)\n",
    "type_item = {}\n",
    "i=1\n",
    "for item in type_dict:\n",
    "    type_item[item] = i\n",
    "    i=i+1\n",
    "\n",
    "    \n",
    "outlet=list(train['Outlet_Size'].value_counts().index)    \n",
    "outlet_size = {}\n",
    "i=1\n",
    "for item in outlet:\n",
    "    outlet_size[item] = i\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "outlet_identifier=list(train['Outlet_Identifier'].value_counts().index)\n",
    "i=1\n",
    "identifier = {}\n",
    "for item in outlet_identifier:\n",
    "    identifier[item] = i\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outlet_location = list(train['Outlet_Location_Type'].value_counts().index)\n",
    "i=1\n",
    "location = {}\n",
    "for item in outlet_location:\n",
    "    location[item] = i\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "    \n",
    "outlet_type = list(train['Outlet_Type'].value_counts().index)\n",
    "i=1\n",
    "type1 ={}\n",
    "for item in outlet_type:\n",
    "    type1[item] = i\n",
    "    i=i+1\n",
    "\n",
    "item_id = list(train['Item_Identifier'].value_counts().index)\n",
    "i=1\n",
    "id_dict ={}\n",
    "for item in item_id:\n",
    "    id_dict[item] = i\n",
    "    i=i+1\n",
    "\n",
    "abc = list(train['Item_Type_Combined'].value_counts().index)\n",
    "i=1\n",
    "abc1 ={}\n",
    "for item in abc:\n",
    "    abc1[item] = i\n",
    "    i=i+1\n",
    "    \n",
    "for dataset in combine:\n",
    "    dataset['Item_Identifier'] = dataset['Item_Identifier'].map(id_dict)\n",
    "    dataset['Item_Fat_Content']=dataset['Item_Fat_Content'].replace(['Low Fat','LF','low fat'], 1)\n",
    "    dataset['Item_Fat_Content']=dataset['Item_Fat_Content'].replace(['Regular','reg'], 2)\n",
    "    dataset['Item_Fat_Content']=dataset['Item_Fat_Content'].replace(['Non-Edible'], 2)\n",
    "    dataset['Item_Type'] = dataset['Item_Type'].map(type_item)\n",
    "    dataset['Outlet_Identifier'] = dataset['Outlet_Identifier'].map(identifier)\n",
    "    dataset['Outlet_Size'] = dataset['Outlet_Size'].map(outlet_size)\n",
    "    dataset['Outlet_Location_Type'] = dataset['Outlet_Location_Type'].map(location)\n",
    "    dataset['Outlet_Type'] = dataset['Outlet_Type'].map(type1)\n",
    "    dataset['Item_Type_Combined'] = dataset['Item_Type_Combined'].map(abc1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imputer import Imputer\n",
    "impute = Imputer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/Imputer.py/imputer/imputer.py:118: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X = X.as_matrix()\n"
     ]
    }
   ],
   "source": [
    "columns = list(train.columns)\n",
    "train_imputed = impute.knn(X=train, column='Item_Weight', k=10)\n",
    "train = pd.DataFrame(train_imputed)\n",
    "train.columns = columns\n",
    "train_imputed = impute.knn(X=train, column='Outlet_Size', k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_imputed)\n",
    "train.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset = dataset.drop(columns=['Item_Outlet_Sales','Item_Type','Outlet_Establishment_Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.drop(columns=['Item_Identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/Imputer.py/imputer/imputer.py:118: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X = X.as_matrix()\n"
     ]
    }
   ],
   "source": [
    "columns = list(test.columns)\n",
    "test_imputed = impute.knn(X=test, column='Item_Weight', k=10)\n",
    "test = pd.DataFrame(test_imputed)\n",
    "test.columns = columns\n",
    "test_imputed = impute.knn(X=test, column='Outlet_Size', k=10)\n",
    "test = pd.DataFrame(test_imputed)\n",
    "test.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = list(train.columns)\n",
    "labels = ['Item_Outlet_Sales']\n",
    "columns.remove('Item_Outlet_Sales')\n",
    "X = train[columns]\n",
    "y = train[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260.9028588180342"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "predict = model.predict(X_test)\n",
    "ans=0\n",
    "for i in range(0,len(y_test.values)):\n",
    "    ans=ans + abs((y_test.values[i][0]-predict[i])/y_test.values[i][0])\n",
    "y_true, y_pred = np.array(y_test), np.array(predict)\n",
    "np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.79094930548673"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(normalize=True)\n",
    "model.fit(X_train, y_train)\n",
    "predict = model.predict(X_test)\n",
    "ans=0\n",
    "for i in range(0,len(y_test.values)):\n",
    "    ans=ans + abs((y_test.values[i][0]-predict[i])/y_test.values[i][0])\n",
    "y_true, y_pred = np.array(y_test), np.array(predict)\n",
    "np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249.2863138133714"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model= RandomForestRegressor(n_estimators=400,max_depth=6, min_samples_leaf=100,n_jobs=4,)\n",
    "model.fit(X_train, y_train)\n",
    "predict = model.predict(X_test)\n",
    "ans=0\n",
    "for i in range(0,len(y_test.values)):\n",
    "    ans=ans + abs((y_test.values[i][0]-predict[i])/y_test.values[i][0])\n",
    "\n",
    "y_true, y_pred = np.array(y_test), np.array(predict)\n",
    "np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260.9028588180342"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, y_pred = np.array(y_test), np.array(predict)\n",
    "np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "mean_sales = train['Item_Outlet_Sales'].mean()\n",
    "from sklearn import cross_validation, metrics\n",
    "def modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "\n",
    "    #Perform cross-validation:\n",
    "    cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain[target], cv=20, scoring='mean_squared_error')\n",
    "    cv_score = np.sqrt(np.abs(cv_score))\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n",
    "    print (\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "    #Predict on testing data:\n",
    "    dtest[target] = alg.predict(dtest[predictors])\n",
    "    \n",
    "    #Export submission file:\n",
    "    IDcol.append(target)\n",
    "    submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2813"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
